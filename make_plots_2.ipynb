{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2200c421",
   "metadata": {},
   "source": [
    "# Training staiblity and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f157edb",
   "metadata": {},
   "source": [
    "## Batch Norm vs Group Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154d18b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import utils.read_runs as rr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_validation_metric_comparison(\n",
    "    run_paths,\n",
    "    run_labels,\n",
    "    column=\"val_loss\",\n",
    "    as_metric=\"identity\",\n",
    "    best_mode=\"best_epoch\",\n",
    "    title=None,\n",
    "    save_path=None,\n",
    "    colors=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot validation metrics for multiple runs with median and IQR.\n",
    "    \n",
    "    Args:\n",
    "        run_paths: List of paths to optuna runs\n",
    "        run_labels: List of labels for each run\n",
    "        column: Column to plot (default \"val_loss\")\n",
    "        as_metric: '1-minus' to convert loss -> Dice, 'identity' for raw values\n",
    "        best_mode: 'final' or 'best_epoch' for selecting best trial\n",
    "        title: Plot title\n",
    "        save_path: Where to save the plot\n",
    "        colors: List of colors for each run\n",
    "    \"\"\"\n",
    "    if colors is None:\n",
    "        colors = [f\"C{i}\" for i in range(len(run_paths))]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6), dpi=150)\n",
    "    \n",
    "    for i, (run_path, label, color) in enumerate(zip(run_paths, run_labels, colors)):\n",
    "        try:\n",
    "            df = rr.get_optuna_df(run_path)\n",
    "            series = df[column].dropna()\n",
    "            \n",
    "            # Collect trials as arrays (variable lengths)\n",
    "            trials = [np.asarray(v, dtype=float) for v in series]\n",
    "            if as_metric == \"1-minus\":\n",
    "                trials = [1.0 - v for v in trials]  # convert Dice loss -> Dice\n",
    "                y_label = \"Validation Soft Dice\"\n",
    "                prefer = \"max\"\n",
    "            else:\n",
    "                y_label = f\"Validation {column.replace('_', ' ').title()}\"\n",
    "                prefer = \"min\"\n",
    "            \n",
    "            max_len = max(len(t) for t in trials)\n",
    "            arr = np.full((len(trials), max_len), np.nan)\n",
    "            for j, t in enumerate(trials):\n",
    "                arr[j, :len(t)] = t\n",
    "            \n",
    "            epochs = np.arange(1, max_len + 1)\n",
    "            median = np.nanmedian(arr, axis=0)\n",
    "            q1 = np.nanpercentile(arr, 25, axis=0)\n",
    "            q3 = np.nanpercentile(arr, 75, axis=0)\n",
    "            \n",
    "            # Plot median line and IQR fill\n",
    "            ax.plot(epochs, median, color=color, label=f\"{label} (median)\", linewidth=2)\n",
    "            ax.fill_between(epochs, q1, q3, color=color, alpha=0.2, label=f\"{label} (IQR)\")\n",
    "            \n",
    "            # Optionally add best trial curve\n",
    "            if best_mode == \"final\":\n",
    "                finals = np.array([row[~np.isnan(row)][-1] if np.any(~np.isnan(row)) else np.nan for row in arr])\n",
    "                best_idx = int(np.nanargmax(finals)) if prefer == \"max\" else int(np.nanargmin(finals))\n",
    "            else:  # best_epoch\n",
    "                agg = np.nanmax(arr, axis=1) if prefer == \"max\" else np.nanmin(arr, axis=1)\n",
    "                best_idx = int(np.nanargmax(agg)) if prefer == \"max\" else int(np.nanargmin(agg))\n",
    "            \n",
    "            best_curve = arr[best_idx, :]\n",
    "            ax.plot(epochs, best_curve, color=color, ls=\"--\", lw=1, alpha=0.7, \n",
    "                   label=f\"{label} (best trial)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load data for {label}: {e}\")\n",
    "    \n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(title or \"Validation Metric Comparison\")\n",
    "    ax.grid(True, alpha=0.25)\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Create output directory\n",
    "fig_dir = Path(\"figs/normalization_comparison\")\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Compare BatchNorm vs GroupNorm for TH prediction\n",
    "th_runs = [\n",
    "    \"\",  \n",
    "    \"\"   \n",
    "]\n",
    "th_labels = [\"TH — BatchNorm\", \"TH — GroupNorm + Anneal\"]\n",
    "th_colors = [\"C0\", \"C1\"]\n",
    "\n",
    "plot_validation_metric_comparison(\n",
    "    run_paths=th_runs,\n",
    "    run_labels=th_labels,\n",
    "    colors=th_colors,\n",
    "    title=\"TH Prediction: BatchNorm vs GroupNorm Comparison\",\n",
    "    save_path=fig_dir / \"th_batchnorm_vs_groupnorm.pdf\"\n",
    ")\n",
    "\n",
    "# Compare BatchNorm vs GroupNorm for NF prediction\n",
    "nf_runs = [\n",
    "    \"\",  # NF BatchNorm\n",
    "    \"\"   # NF GroupNorm + anneal\n",
    "]\n",
    "nf_labels = [\"NF — BatchNorm\", \"NF — GroupNorm + Anneal\"]\n",
    "nf_colors = [\"C2\", \"C3\"]\n",
    "\n",
    "plot_validation_metric_comparison(\n",
    "    run_paths=nf_runs,\n",
    "    run_labels=nf_labels,\n",
    "    colors=nf_colors,\n",
    "    title=\"NF Prediction: BatchNorm vs GroupNorm Comparison\",\n",
    "    save_path=fig_dir / \"nf_batchnorm_vs_groupnorm.pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de15ea3",
   "metadata": {},
   "source": [
    "## anneal and lr behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2763164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "def plot_trial_analysis_separate(\n",
    "    trial_path, \n",
    "    figsize=(10, 6),\n",
    "    save_dir=None,                 # e.g. \".../trial_1/plots\"\n",
    "    save_formats=(\"png\",),         # e.g. (\"png\",\"pdf\")\n",
    "    dpi=300,\n",
    "    show=True,\n",
    "    close=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Create 4 separate plots for a single trial and optionally save them.\n",
    "\n",
    "    Returns:\n",
    "        figures, weight_lists, saved_paths\n",
    "        - figures: list[Figure]\n",
    "        - weight_lists: dict of lists\n",
    "        - saved_paths: dict[str, list[str]] mapping short name -> list of saved files\n",
    "    \"\"\"\n",
    "    # Load metadata\n",
    "    metadata_path = os.path.join(trial_path, \"metadata.json\")\n",
    "    if not os.path.exists(metadata_path):\n",
    "        raise FileNotFoundError(f\"metadata.json not found in {trial_path}\")\n",
    "    with open(metadata_path, \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    trial_name = os.path.basename(os.path.normpath(trial_path))\n",
    "    figures = []\n",
    "    saved_paths = {\"lr\": [], \"weights\": [], \"comp\": [], \"comp_weighted\": []}\n",
    "\n",
    "    # Ensure output dir\n",
    "    if save_dir is not None:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 1) Learning rate\n",
    "    fig1 = plt.figure(figsize=figsize); ax1 = fig1.add_subplot(111)\n",
    "    plot_learning_rate(ax1, metadata, trial_name)\n",
    "    fig1.tight_layout(); figures.append(fig1)\n",
    "    if save_dir:\n",
    "        for fmt in save_formats:\n",
    "            p = os.path.join(save_dir, f\"{trial_name}_learning_rate.{fmt}\")\n",
    "            fig1.savefig(p, dpi=dpi, bbox_inches=\"tight\"); saved_paths[\"lr\"].append(p)\n",
    "    if show: plt.show()\n",
    "    if close: plt.close(fig1)\n",
    "\n",
    "    # 2) Loss weights\n",
    "    fig2 = plt.figure(figsize=figsize); ax2 = fig2.add_subplot(111)\n",
    "    weight_lists = plot_loss_weights(ax2, metadata, trial_name)\n",
    "    fig2.tight_layout(); figures.append(fig2)\n",
    "    if save_dir:\n",
    "        for fmt in save_formats:\n",
    "            p = os.path.join(save_dir, f\"{trial_name}_loss_weights.{fmt}\")\n",
    "            fig2.savefig(p, dpi=dpi, bbox_inches=\"tight\"); saved_paths[\"weights\"].append(p)\n",
    "    if show: plt.show()\n",
    "    if close: plt.close(fig2)\n",
    "\n",
    "    # 3) Component losses (unweighted)\n",
    "    fig3 = plt.figure(figsize=figsize); ax3 = fig3.add_subplot(111)\n",
    "    plot_component_losses(ax3, metadata, trial_name)\n",
    "    fig3.tight_layout(); figures.append(fig3)\n",
    "    if save_dir:\n",
    "        for fmt in save_formats:\n",
    "            p = os.path.join(save_dir, f\"{trial_name}_component_losses.{fmt}\")\n",
    "            fig3.savefig(p, dpi=dpi, bbox_inches=\"tight\"); saved_paths[\"comp\"].append(p)\n",
    "    if show: plt.show()\n",
    "    if close: plt.close(fig3)\n",
    "\n",
    "    # 4) Weighted component losses\n",
    "    fig4 = plt.figure(figsize=figsize); ax4 = fig4.add_subplot(111)\n",
    "    plot_weighted_component_losses(ax4, metadata, weight_lists, trial_name)\n",
    "    fig4.tight_layout(); figures.append(fig4)\n",
    "    if save_dir:\n",
    "        for fmt in save_formats:\n",
    "            p = os.path.join(save_dir, f\"{trial_name}_weighted_component_losses.{fmt}\")\n",
    "            fig4.savefig(p, dpi=dpi, bbox_inches=\"tight\"); saved_paths[\"comp_weighted\"].append(p)\n",
    "    if show: plt.show()\n",
    "    if close: plt.close(fig4)\n",
    "\n",
    "    return figures, weight_lists, saved_paths\n",
    "\n",
    "def plot_learning_rate(ax, metadata, trial_name):\n",
    "    lrs = metadata.get(\"lrs\", [])\n",
    "    epochs = list(range(len(lrs)))\n",
    "    ax.plot(epochs, lrs, 'b-', linewidth=2, label='Learning Rate')\n",
    "    ax.set_xlabel('Epoch'); ax.set_ylabel('Learning Rate')\n",
    "    ax.set_title(f'Learning Rate Schedule - {trial_name}')\n",
    "    if len(lrs) and all(v > 0 for v in lrs):\n",
    "        ax.set_yscale('log')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Mark annealing events\n",
    "    for event in metadata.get(\"loss_weight_history\", []):\n",
    "        ep = event.get(\"epoch\", None)\n",
    "        if ep is not None and 0 <= ep < len(lrs):\n",
    "            ax.axvline(x=ep, color='red', linestyle='--', alpha=0.7)\n",
    "            ax.text(ep, lrs[ep], f'Anneal\\n@{ep}', rotation=90, ha='right', va='bottom', fontsize=8)\n",
    "    ax.legend()\n",
    "\n",
    "def plot_loss_weights(ax, metadata, trial_name):\n",
    "    initial_w_dice = metadata.get(\"w_dice\", 0.6)\n",
    "    initial_w_cldice = metadata.get(\"w_cldice\", 0.2)\n",
    "    initial_w_bce = metadata.get(\"w_bce\", 0.2)\n",
    "\n",
    "    epochs_run = metadata.get(\"epochs_run\", 0)\n",
    "    epochs = list(range(epochs_run))\n",
    "\n",
    "    w_dice_list = [initial_w_dice] * epochs_run\n",
    "    w_cldice_list = [initial_w_cldice] * epochs_run\n",
    "    w_bce_list  = [initial_w_bce]  * epochs_run\n",
    "\n",
    "    for event in metadata.get(\"loss_weight_history\", []):\n",
    "        ep = event.get(\"epoch\", None)\n",
    "        if ep is None: \n",
    "            continue\n",
    "        new_w_dice = event.get(\"w_dice\", w_dice_list[ep])\n",
    "        new_w_cldice = event.get(\"w_cldice\", w_cldice_list[ep])\n",
    "        new_w_bce = event.get(\"w_bce\", w_bce_list[ep])\n",
    "        for i in range(ep, epochs_run):\n",
    "            w_dice_list[i] = new_w_dice\n",
    "            w_cldice_list[i] = new_w_cldice\n",
    "            w_bce_list[i] = new_w_bce\n",
    "\n",
    "    ax.plot(epochs, w_dice_list, 'r-', linewidth=2, label='Dice Weight')\n",
    "    ax.plot(epochs, w_cldice_list, 'g-', linewidth=2, label='clDice Weight')\n",
    "    ax.plot(epochs, w_bce_list,  'b-', linewidth=2, label='BCE Weight')\n",
    "    ax.set_xlabel('Epoch'); ax.set_ylabel('Weight')\n",
    "    ax.set_title(f'Loss Component Weights - {trial_name}')\n",
    "    ax.grid(True, alpha=0.3); ax.legend()\n",
    "\n",
    "    for event in metadata.get(\"loss_weight_history\", []):\n",
    "        ep = event.get(\"epoch\", None)\n",
    "        if ep is not None:\n",
    "            ax.axvline(x=ep, color='black', linestyle='--', alpha=0.7)\n",
    "\n",
    "    return {'w_dice': w_dice_list, 'w_cldice': w_cldice_list, 'w_bce': w_bce_list}\n",
    "\n",
    "def plot_component_losses(ax, metadata, trial_name):\n",
    "    comp_loss = metadata.get(\"comp_loss\", [])\n",
    "    if not comp_loss:\n",
    "        ax.text(0.5, 0.5, 'No component loss data', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(f'Component Losses (Unweighted) - {trial_name}')\n",
    "        return\n",
    "    epochs = list(range(len(comp_loss)))\n",
    "    dice_losses   = [e[\"dice\"]   for e in comp_loss]\n",
    "    cldice_losses = [e[\"cldice\"] for e in comp_loss]\n",
    "    bce_losses    = [e[\"bce\"]    for e in comp_loss]\n",
    "    ax.plot(epochs, dice_losses,   'r-', linewidth=2, label='Dice Loss')\n",
    "    ax.plot(epochs, cldice_losses, 'g-', linewidth=2, label='clDice Loss')\n",
    "    ax.plot(epochs, bce_losses,    'b-', linewidth=2, label='BCE Loss')\n",
    "    ax.set_xlabel('Epoch'); ax.set_ylabel('Loss')\n",
    "    ax.set_title(f'Component Losses (Unweighted) - {trial_name}')\n",
    "    ax.grid(True, alpha=0.3); ax.legend()\n",
    "\n",
    "def plot_weighted_component_losses(ax, metadata, weight_lists, trial_name):\n",
    "    comp_loss = metadata.get(\"comp_loss\", [])\n",
    "    if not comp_loss or not weight_lists:\n",
    "        ax.text(0.5, 0.5, 'No data available', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(f'Weighted Component Losses - {trial_name}')\n",
    "        return\n",
    "    n = len(comp_loss)\n",
    "    epochs = list(range(n))\n",
    "    wd, wc, wb = weight_lists['w_dice'], weight_lists['w_cldice'], weight_lists['w_bce']\n",
    "    m = min(n, len(wd), len(wc), len(wb))\n",
    "\n",
    "    weighted_dice   = [comp_loss[i][\"dice\"]   * wd[i] for i in range(m)]\n",
    "    weighted_cldice = [comp_loss[i][\"cldice\"] * wc[i] for i in range(m)]\n",
    "    weighted_bce    = [comp_loss[i][\"bce\"]    * wb[i] for i in range(m)]\n",
    "\n",
    "    ax.plot(epochs[:m], weighted_dice,   'r-', linewidth=2, label='Weighted Dice Loss')\n",
    "    ax.plot(epochs[:m], weighted_cldice, 'g-', linewidth=2, label='Weighted clDice Loss')\n",
    "    ax.plot(epochs[:m], weighted_bce,    'b-', linewidth=2, label='Weighted BCE Loss')\n",
    "    ax.set_xlabel('Epoch'); ax.set_ylabel('Weighted Loss')\n",
    "    ax.set_title(f'Weighted Component Losses - {trial_name}')\n",
    "    ax.grid(True, alpha=0.3); ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ec6cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "anneal_fig_dir = Path(\"figs/anneal_vs_dice_comparison\")\n",
    "anneal_fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ⬇️ Same trial path as your original example — NOT changed\n",
    "figs, weights, saved = plot_trial_analysis_separate(\n",
    "    trial_path=\"\",\n",
    "    save_dir=anneal_fig_dir,       # only the save location changes\n",
    "    save_formats=(\"pdf\",),         # or (\"png\",\"pdf\")\n",
    "    show=True,\n",
    "    close=True\n",
    ")\n",
    "print(saved)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34378330",
   "metadata": {},
   "source": [
    "## With and without peripherin difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe2757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_th_experiments_with_without_peripherin(figsize=(12, 8), save_path=None):\n",
    "    \"\"\"\n",
    "    Compare NF prediction experiments with and without peripherin channel.\n",
    "    \n",
    "    Args:\n",
    "        figsize (tuple): Figure size\n",
    "        save_path (str): Optional path to save the plot\n",
    "    \"\"\"\n",
    "    # Define the two experiments\n",
    "    with_peripherin_path = \"\"  # 37 channels (with peripherin)\n",
    "    without_peripherin_path = \"\"  # 36 channels (without peripherin)\n",
    "    \n",
    "    # Get validation curves for both experiments\n",
    "    with_peripherin_mean = get_mean_validation_curve(with_peripherin_path)\n",
    "    without_peripherin_mean = get_mean_validation_curve(without_peripherin_path)\n",
    "    \n",
    "    # Create the comparison plot\n",
    "    fig, ax = plt.subplots(figsize=figsize, dpi=150)\n",
    "    \n",
    "    # Plot both curves\n",
    "    epochs_with = list(range(1, len(with_peripherin_mean) + 1))\n",
    "    epochs_without = list(range(1, len(without_peripherin_mean) + 1))\n",
    "    \n",
    "    ax.plot(epochs_with, with_peripherin_mean, 'b-', linewidth=2.5, \n",
    "            label='NF with Peripherin (37 channels)', alpha=0.8)\n",
    "    ax.plot(epochs_without, without_peripherin_mean, 'r-', linewidth=2.5, \n",
    "            label='NF without Peripherin (36 channels)', alpha=0.8)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Mean Validation Loss', fontsize=12)\n",
    "    ax.set_title('NF Prediction: With vs Without Peripherin Channel', fontsize=14, pad=20)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=11, loc='upper right')\n",
    "    \n",
    "    # Add some statistics\n",
    "    final_with = with_peripherin_mean[-1] if len(with_peripherin_mean) > 0 else float('nan')\n",
    "    final_without = without_peripherin_mean[-1] if len(without_peripherin_mean) > 0 else float('nan')\n",
    "    \n",
    "    # Add text box with final values\n",
    "    textstr = f'Final validation loss:\\nWith Peripherin: {final_with:.4f}\\nWithout Peripherin: {final_without:.4f}'\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "    ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=10,\n",
    "            verticalalignment='top', bbox=props)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'with_peripherin_mean': with_peripherin_mean,\n",
    "        'without_peripherin_mean': without_peripherin_mean,\n",
    "        'final_with': final_with,\n",
    "        'final_without': final_without\n",
    "    }\n",
    "\n",
    "def get_mean_validation_curve(run_path):\n",
    "    \"\"\"\n",
    "    Get the mean validation curve across all trials in a run.\n",
    "    \n",
    "    Args:\n",
    "        run_path (str): Path to the optuna run directory\n",
    "        \n",
    "    Returns:\n",
    "        list: Mean validation loss at each epoch across all trials\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the run data\n",
    "        df = rr.get_optuna_df(run_path)\n",
    "        \n",
    "        if df.empty:\n",
    "            print(f\"Warning: No data found in {run_path}\")\n",
    "            return []\n",
    "        \n",
    "        # Extract validation loss series\n",
    "        val_loss_series = df['val_loss'].dropna()\n",
    "        \n",
    "        if val_loss_series.empty:\n",
    "            print(f\"Warning: No validation loss data found in {run_path}\")\n",
    "            return []\n",
    "        \n",
    "        # Convert to list of arrays (each trial can have different length)\n",
    "        trials = []\n",
    "        for val_loss_list in val_loss_series:\n",
    "            if isinstance(val_loss_list, list) and len(val_loss_list) > 0:\n",
    "                trials.append(np.array(val_loss_list, dtype=float))\n",
    "        \n",
    "        if not trials:\n",
    "            print(f\"Warning: No valid trials found in {run_path}\")\n",
    "            return []\n",
    "        \n",
    "        # Find the maximum length across all trials\n",
    "        max_epochs = max(len(trial) for trial in trials)\n",
    "        \n",
    "        # Create array to hold all trials (padded with NaN)\n",
    "        all_trials = np.full((len(trials), max_epochs), np.nan)\n",
    "        for i, trial in enumerate(trials):\n",
    "            all_trials[i, :len(trial)] = trial\n",
    "        \n",
    "        # Calculate mean across trials at each epoch (ignoring NaN values)\n",
    "        mean_curve = np.nanmean(all_trials, axis=0)\n",
    "        \n",
    "        # Remove trailing NaN values\n",
    "        valid_indices = ~np.isnan(mean_curve)\n",
    "        if np.any(valid_indices):\n",
    "            last_valid = np.where(valid_indices)[0][-1]\n",
    "            mean_curve = mean_curve[:last_valid + 1]\n",
    "        \n",
    "        print(f\"Processed {len(trials)} trials from {run_path}\")\n",
    "        print(f\"Mean curve length: {len(mean_curve)} epochs\")\n",
    "        \n",
    "        return mean_curve.tolist()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {run_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Run the comparison\n",
    "results = compare_th_experiments_with_without_peripherin(\n",
    "    save_path=\"figs/th_with_without_peripherin_comparison.pdf\"\n",
    ")\n",
    "\n",
    "# Print some summary statistics\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"With Peripherin - Final loss: {results['final_with']:.4f}\")\n",
    "print(f\"Without Peripherin - Final loss: {results['final_without']:.4f}\")\n",
    "if not np.isnan(results['final_with']) and not np.isnan(results['final_without']):\n",
    "    difference = results['final_with'] - results['final_without']\n",
    "    print(f\"Difference (With - Without): {difference:.4f}\")\n",
    "    if difference > 0:\n",
    "        print(\"Without peripherin performs better (lower loss)\")\n",
    "    else:\n",
    "        print(\"With peripherin performs better (lower loss)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b524e9",
   "metadata": {},
   "source": [
    "# Peripherin plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bf35dd",
   "metadata": {},
   "source": [
    "## Anneal vs no anneal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f5587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "def plot_validation_metric_with_continuation(\n",
    "    run_path,\n",
    "    continuation_paths=None,  \n",
    "    dice_only_run_path=None,  \n",
    "    column=\"val_loss\",\n",
    "    as_metric=\"identity\",\n",
    "    best_mode=\"best_hard_dice\",  \n",
    "    title=None,\n",
    "    save_path=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot validation metric with optional continuation phases and dice-only comparison.\n",
    "    continuation_paths: dict mapping continuation labels to their directories\n",
    "    dice_only_run_path: path to standalone dice-only run (e.g., peripherin_scratch_2025-08-19_08-20-01)\n",
    "    \"\"\"\n",
    "    # Get original run data\n",
    "    df = rr.get_optuna_df(run_path)\n",
    "    series = df[column].dropna()\n",
    "\n",
    "    # Collect trials as arrays (variable lengths)\n",
    "    trials = [np.asarray(v, dtype=float) for v in series]\n",
    "    if as_metric == \"1-minus\":\n",
    "        trials = [1.0 - v for v in trials]\n",
    "        y_label = \"Validation Soft Dice\"\n",
    "        prefer = \"max\"\n",
    "    else:\n",
    "        y_label = f\"Validation {column.replace('_', ' ').title()}\"\n",
    "        prefer = \"min\"  # for loss, we want minimum\n",
    "\n",
    "    max_len = max(len(t) for t in trials)\n",
    "    \n",
    "    # Load continuation data if provided\n",
    "    continuation_data = {}\n",
    "    if continuation_paths:\n",
    "        for cont_label, cont_path in continuation_paths.items():\n",
    "            try:\n",
    "                cont_meta_path = Path(cont_path) / \"metadata_dice_continuation.json\"\n",
    "                if cont_meta_path.exists():\n",
    "                    cont_meta = json.loads(cont_meta_path.read_text())\n",
    "                    cont_losses = cont_meta.get(\"training_curves\", {}).get(\"val_losses\", [])\n",
    "                    if cont_losses:\n",
    "                        if as_metric == \"1-minus\":\n",
    "                            cont_losses = [1.0 - v for v in cont_losses]\n",
    "                        continuation_data[cont_label] = np.array(cont_losses)\n",
    "                        print(f\"Loaded {len(cont_losses)} continuation epochs for {cont_label}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load continuation data for {cont_label}: {e}\")\n",
    "\n",
    "    dice_only_data = None\n",
    "    if dice_only_run_path:\n",
    "        try:\n",
    "            dice_df = rr.get_optuna_df(dice_only_run_path)\n",
    "            dice_series = dice_df[column].dropna()\n",
    "            dice_trials = [np.asarray(v, dtype=float) for v in dice_series]\n",
    "            if as_metric == \"1-minus\":\n",
    "                dice_trials = [1.0 - v for v in dice_trials]\n",
    "            \n",
    "            # Create array for dice-only trials\n",
    "            dice_max_len = max(len(t) for t in dice_trials) if dice_trials else 0\n",
    "            dice_arr = np.full((len(dice_trials), dice_max_len), np.nan)\n",
    "            for i, t in enumerate(dice_trials):\n",
    "                dice_arr[i, :len(t)] = t\n",
    "            \n",
    "            # Calculate median curve instead of selecting best trial\n",
    "            dice_only_data = np.nanmedian(dice_arr, axis=0)\n",
    "            print(f\"Loaded dice-only median curve: {len(dice_only_data)} epochs from {len(dice_trials)} trials\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load dice-only run data: {e}\")\n",
    "\n",
    "    # Find the best trial using hard dice metric\n",
    "    arr = np.full((len(trials), max_len), np.nan)\n",
    "    for i, t in enumerate(trials):\n",
    "        arr[i, :len(t)] = t\n",
    "\n",
    "    if best_mode == \"best_hard_dice\":\n",
    "        # Use the same selection as get_best_trial() - based on val_dice_at_best_thr\n",
    "        periph_df = rr.get_peripherin_df(run_path)\n",
    "        if periph_df is not None and \"val_dice_at_best_thr\" in periph_df.columns:\n",
    "            best_trial_name = periph_df[\"val_dice_at_best_thr\"].astype(float).idxmax()\n",
    "            # Extract trial number from name like \"trial_15\"\n",
    "            if best_trial_name.startswith(\"trial_\"):\n",
    "                best_idx = int(best_trial_name.split(\"_\")[1])\n",
    "            else:\n",
    "                # Fallback to index position\n",
    "                best_idx = periph_df.index.get_loc(best_trial_name)\n",
    "            print(f\"Selected trial {best_trial_name} (index {best_idx}) based on val_dice_at_best_thr\")\n",
    "        else:\n",
    "            # Fallback to best epoch method\n",
    "            agg = np.nanmax(arr, axis=1) if prefer == \"max\" else np.nanmin(arr, axis=1)\n",
    "            best_idx = int(np.nanargmax(agg)) if prefer == \"max\" else int(np.nanargmin(agg))\n",
    "            print(f\"Fallback: Selected trial {best_idx} based on best epoch\")\n",
    "    elif best_mode == \"final\":\n",
    "        finals = np.array([row[~np.isnan(row)][-1] if np.any(~np.isnan(row)) else np.nan for row in arr])\n",
    "        best_idx = int(np.nanargmax(finals)) if prefer == \"max\" else int(np.nanargmin(finals))\n",
    "    else:  # best_epoch\n",
    "        agg = np.nanmax(arr, axis=1) if prefer == \"max\" else np.nanmin(arr, axis=1)\n",
    "        best_idx = int(np.nanargmax(agg)) if prefer == \"max\" else int(np.nanargmin(agg))\n",
    "    \n",
    "    best_curve = arr[best_idx, :]\n",
    "    \n",
    "    # Find the actual end point of the best trial (where it stopped)\n",
    "    best_trial_valid_epochs = np.where(~np.isnan(best_curve))[0]\n",
    "    if len(best_trial_valid_epochs) > 0:\n",
    "        best_trial_end_epoch = best_trial_valid_epochs[-1] + 1  # +1 for 1-based indexing\n",
    "        best_trial_final_loss = best_curve[best_trial_valid_epochs[-1]]\n",
    "    else:\n",
    "        best_trial_end_epoch = 1\n",
    "        best_trial_final_loss = np.nan\n",
    "    \n",
    "    print(f\"Best trial ended at epoch {best_trial_end_epoch}\")\n",
    "    \n",
    "    # Calculate statistics for original runs\n",
    "    epochs = np.arange(1, max_len + 1)\n",
    "    median = np.nanmedian(arr, axis=0)\n",
    "    q1 = np.nanpercentile(arr, 25, axis=0)\n",
    "    q3 = np.nanpercentile(arr, 75, axis=0)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 4), dpi=150)\n",
    "    \n",
    "    # Original run curves\n",
    "    ax.plot(epochs, median, color=\"C0\", label=\"Median (original)\", linewidth=2)\n",
    "    ax.fill_between(epochs, q1, q3, color=\"C0\", alpha=0.2, label=\"IQR (original)\")\n",
    "    ax.plot(epochs, best_curve, color=\"C1\", ls=\"--\", lw=1.5, label=f\"Best trial (#{best_idx})\")\n",
    "\n",
    "    # Add dice-only standalone curve if provided\n",
    "    if dice_only_data is not None:\n",
    "        dice_epochs = np.arange(1, len(dice_only_data) + 1)\n",
    "        ax.plot(dice_epochs, dice_only_data, color=\"C4\", linewidth=2, \n",
    "               label=\"Dice-only (median)\", linestyle=\"-.\")\n",
    "\n",
    "    # Add continuation data starting from the actual end of the best trial\n",
    "    colors = [\"C2\", \"C3\"]  # Reserve C4 for dice-only standalone\n",
    "    \n",
    "    for i, (cont_label, cont_curve) in enumerate(continuation_data.items()):\n",
    "        if len(cont_curve) > 0:\n",
    "            # Start continuation from where the best trial actually ended\n",
    "            cont_epochs = np.arange(best_trial_end_epoch + 1, best_trial_end_epoch + 1 + len(cont_curve))\n",
    "            color = colors[i % len(colors)]\n",
    "            \n",
    "            # Connect the curves at transition point\n",
    "            if not np.isnan(best_trial_final_loss):\n",
    "                # Draw connecting line\n",
    "                ax.plot([best_trial_end_epoch, best_trial_end_epoch + 1], \n",
    "                       [best_trial_final_loss, cont_curve[0]], \n",
    "                       color=color, linestyle=\":\", alpha=0.7)\n",
    "            \n",
    "            # Plot continuation (single curve, no ribbon since it's just one trial)\n",
    "            ax.plot(cont_epochs, cont_curve, color=color, linewidth=2,\n",
    "                   label=f\"{cont_label} continuation\")\n",
    "            \n",
    "            # Mark transition point\n",
    "            if i == 0:  # Only add the label once\n",
    "                ax.axvline(best_trial_end_epoch + 0.5, color=\"red\", linestyle=\"--\", \n",
    "                          alpha=0.7, linewidth=1, label=\"Dice-only start\")\n",
    "\n",
    "    # After plotting everything but before plt.tight_layout()\n",
    "    # Legend inside upper right\n",
    "\n",
    "\n",
    "    # --- Add text box with final values ---\n",
    "    final_texts = []\n",
    "\n",
    "    # Final value of best trial\n",
    "    if not np.isnan(best_trial_final_loss):\n",
    "        final_texts.append(f\"Pre-continuation lowest: {min(best_curve[best_trial_valid_epochs]):.4f}\")\n",
    "\n",
    "    # Dice-only run final value\n",
    "    if dice_only_data is not None and len(dice_only_data) > 0:\n",
    "        final_texts.append(f\"Dice-only lowest: {min(dice_only_data):.4f}\")\n",
    "\n",
    "    # Continuation(s) final values\n",
    "    for cont_label, cont_curve in continuation_data.items():\n",
    "        if len(cont_curve) > 0:\n",
    "            final_texts.append(f\"Continuation lowest: {min(cont_curve):.4f}\")\n",
    "\n",
    "    if final_texts:\n",
    "        textstr = \"Best validation:\\n\" + \"\\n\".join(final_texts)\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "        ax.text(\n",
    "            0.3, 0.98, textstr,   \n",
    "            transform=ax.transAxes,\n",
    "            fontsize=9,\n",
    "            verticalalignment='top',\n",
    "            horizontalalignment='left',\n",
    "            bbox=props\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(title or f\"{Path(run_path).name} with continuations\")\n",
    "    ax.grid(True, alpha=0.25)\n",
    "    ax.legend(loc=\"upper right\", frameon=True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "    return fig, ax\n",
    "\n",
    "# Create output directory\n",
    "anneal_fig_dir = Path(\"figs/anneal_vs_dice_comparison\")\n",
    "anneal_fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1. Scratch: Annealed vs Dice-only\n",
    "plot_validation_metric_with_continuation(\n",
    "    run_path=\"\",\n",
    "    continuation_paths={\n",
    "        \"dice_only\": \"\"\n",
    "    },\n",
    "    dice_only_run_path=\"\",\n",
    "    column=\"val_loss\",\n",
    "    best_mode=\"best_hard_dice\",\n",
    "    title=\"PRPH: Scratch — Annealed vs Dice-only\",\n",
    "    save_path=anneal_fig_dir / \"scratch_annealed_vs_dice_only.pdf\"\n",
    ")\n",
    "\n",
    "# 2. TH Transfer: Annealed vs Dice-only  \n",
    "plot_validation_metric_with_continuation(\n",
    "    run_path=\"\",\n",
    "    continuation_paths={\n",
    "        \"dice_only\": \"\"\n",
    "    },\n",
    "    dice_only_run_path=\"\",\n",
    "    column=\"val_loss\",\n",
    "    best_mode=\"best_hard_dice\",\n",
    "    title=\"PRPH: TH Transfer — Annealed vs Dice-only\",\n",
    "    save_path=anneal_fig_dir / \"th_transfer_annealed_vs_dice_only.pdf\"\n",
    ")\n",
    "\n",
    "# 3. NF Transfer: Annealed vs Dice-only\n",
    "plot_validation_metric_with_continuation(\n",
    "    run_path=\"\",\n",
    "    continuation_paths={\n",
    "        \"dice_only\": \"\"\n",
    "    },\n",
    "    dice_only_run_path=\"\",\n",
    "    column=\"val_loss\",\n",
    "    best_mode=\"best_hard_dice\",\n",
    "    title=\"PRPH: NF Transfer — Annealed vs Dice-only\",\n",
    "    save_path=anneal_fig_dir / \"nf_transfer_annealed_vs_dice_only.pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d7b468",
   "metadata": {},
   "source": [
    "## Anneal vs soft_dice difference plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299fd3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PRPH annealed(continued) vs dice-only: delta histogram + zoomed scatter with highlights ===\n",
    "# Outputs:\n",
    "#   figs/anneal_vs_dice/delta_hist_{ctx}_{metric}.pdf\n",
    "#   figs/anneal_vs_dice/scatter_{ctx}_{metric}_zoom.pdf\n",
    "#   figs/anneal_vs_dice/highlighted_{ctx}_{metric}.csv\n",
    "#\n",
    "# Contexts use \"*_continued\" as the annealed variant, per your preference.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "CSV_PATH = \"per_sample_metrics.csv\"\n",
    "METRIC   = \"soft_dice\"    # or \"cldice\"\n",
    "METRIC_LABEL = \"clDice\" if METRIC == \"cldice\" else METRIC.replace(\"_\", \" \").title()\n",
    "\n",
    "# Pair the runs you want to compare (annealed=continued vs pure dice).\n",
    "CONTEXTS = {\n",
    "    \"scratch\": {\"annealed\": \"scratch_continued\", \"dice\": \"scratch_dice_only\"},\n",
    "    \"nf\":      {\"annealed\": \"nf_continued\",      \"dice\": \"nf_dice_only\"},\n",
    "    \"th\":      {\"annealed\": \"th_continued\",      \"dice\": \"th_dice_only\"},\n",
    "}\n",
    "\n",
    "# Highlighting controls for scatter\n",
    "TOP_K_ABS_DELTA = 12        # show top-|Δ| cases\n",
    "DELTA_THRESHOLD = 0.05      # also highlight any |Δ| >= this threshold\n",
    "SHOW_LABELS     = True      # annotate highlighted points with sample_idx\n",
    "\n",
    "# Bootstrap for CI\n",
    "N_BOOT          = 10_000\n",
    "SEED            = 42\n",
    "\n",
    "# Output directory\n",
    "out_dir = Path(\"figs/anneal_vs_dice\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "# ------------------------------------------------\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _pair_xy(df, annealed_label, dice_label, metric):\n",
    "    g = df.groupby([\"sample_idx\", \"model_label\"], as_index=False).agg({metric: \"mean\"})\n",
    "    wide = g[g[\"model_label\"].isin([annealed_label, dice_label])].pivot(\n",
    "        index=\"sample_idx\", columns=\"model_label\", values=metric\n",
    "    ).dropna(subset=[annealed_label, dice_label])\n",
    "    x = wide[annealed_label].to_numpy()   # Annealed (continued)\n",
    "    y = wide[dice_label].to_numpy()       # Dice-only\n",
    "    return x, y, wide.index.values\n",
    "\n",
    "def _bootstrap_mean_ci(x, n_boot=N_BOOT, ci=95, seed=SEED):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    boots = rng.choice(x, size=(n_boot, x.size), replace=True).mean(axis=1)\n",
    "    lo = np.percentile(boots, (100-ci)/2)\n",
    "    hi = np.percentile(boots, 100-(100-ci)/2)\n",
    "    return x.mean(), (lo, hi)\n",
    "\n",
    "def _legend_and_textbox(ax, text_lines, kind=\"scatter\"):\n",
    "    \"\"\"\n",
    "    kind = \"hist\" or \"scatter\"\n",
    "    - Legend is always loc='upper right'\n",
    "    - Text placement:\n",
    "        * hist: top-right, below legend\n",
    "        * scatter: top-left\n",
    "    \"\"\"\n",
    "    loc = \"upper left\" if kind == \"scatter\" else \"upper right\"\n",
    "    ax.legend(loc=loc, frameon=True)\n",
    "\n",
    "    if not text_lines:\n",
    "        return\n",
    "\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "    text = \"\\n\".join(text_lines)\n",
    "\n",
    "    if kind == \"hist\":\n",
    "        # Place textbox below legend area; tweak y-coord if needed for your legend height\n",
    "        ax.text(\n",
    "            0.98, 0.80, text,\n",
    "            transform=ax.transAxes, fontsize=9,\n",
    "            va='top', ha='right', bbox=props\n",
    "        )\n",
    "    else:\n",
    "        # scatter: top-left\n",
    "        ax.text(\n",
    "            0.02, 0.80, text,\n",
    "            transform=ax.transAxes, fontsize=9,\n",
    "            va='top', ha='left', bbox=props\n",
    "        )\n",
    "\n",
    "# ---------- plotters ----------\n",
    "def plot_delta_hist(diffs, out_path, title=\"\"):\n",
    "    \"\"\"\n",
    "    diffs: Dice-only − Annealed (continued)\n",
    "    \"\"\"\n",
    "    mean_delta, (lo, hi) = _bootstrap_mean_ci(diffs)\n",
    "    fig, ax = plt.subplots(figsize=(6.2, 4.2), dpi=130)\n",
    "    ax.hist(diffs, bins=15, alpha=0.8, edgecolor='black')\n",
    "    ax.axvline(0, linestyle=\"--\", linewidth=1.0, color='red', alpha=0.9, label='Δ = 0')\n",
    "    ax.axvline(mean_delta, linewidth=1.2, color='blue', label=f'Mean Δ = {mean_delta:.3f}')\n",
    "    ax.set_xlabel(f\"Δ {METRIC_LABEL} (Dice-only − Annealed)\")\n",
    "    ax.set_ylabel(\"Number of Slides\")\n",
    "    ax.set_title(title)  # keep blank or minimal to avoid clutter\n",
    "    _legend_and_textbox(ax, [\n",
    "        \"Summary:\",\n",
    "        f\"Mean Δ: {mean_delta:.3f}\",\n",
    "        f\"95% CI: [{lo:.3f}, {hi:.3f}]\",\n",
    "        f\"n = {diffs.size}\",\n",
    "    ], kind=\"hist\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, dpi=200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved:\", out_path)\n",
    "\n",
    "def plot_scatter_zoom(x, y, sample_ids, out_path, title=\"\", pad=0.02,\n",
    "                      top_k=TOP_K_ABS_DELTA, thr=DELTA_THRESHOLD, show_labels=SHOW_LABELS):\n",
    "    # Axis range + identity\n",
    "    xmin, xmax = x.min(), x.max()\n",
    "    ymin, ymax = y.min(), y.max()\n",
    "    lo = min(xmin, ymin); hi = max(xmax, ymax)\n",
    "    span = max(hi - lo, 1e-6)\n",
    "    lo -= pad*span; hi += pad*span\n",
    "\n",
    "    deltas = y - x  # Dice-only − Annealed\n",
    "    idx_sorted = np.argsort(-np.abs(deltas))  # descending by |Δ|\n",
    "    top_idx = set(idx_sorted[:top_k])\n",
    "    thr_idx = set(np.where(np.abs(deltas) >= thr)[0])\n",
    "    highlight_idx = np.array(sorted(top_idx.union(thr_idx)), dtype=int)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6.2, 6.2), dpi=130)\n",
    "\n",
    "    # Base scatter (all)\n",
    "    ax.scatter(x, y, alpha=0.25, edgecolor=\"none\", label=\"All slides\")\n",
    "\n",
    "    # Highlight improvements (Δ>0) and degradations (Δ<0)\n",
    "    if highlight_idx.size > 0:\n",
    "        imp = highlight_idx[deltas[highlight_idx] > 0]\n",
    "        deg = highlight_idx[deltas[highlight_idx] < 0]\n",
    "\n",
    "        if imp.size:\n",
    "            ax.scatter(x[imp], y[imp], alpha=0.9, edgecolor=\"black\", linewidths=0.6,\n",
    "                       label=f\"Highlighted (Δ>0, n={imp.size})\")\n",
    "        if deg.size:\n",
    "            ax.scatter(x[deg], y[deg], alpha=0.9, edgecolor=\"black\", linewidths=0.6,\n",
    "                       label=f\"Highlighted (Δ<0, n={deg.size})\", marker=\"s\")\n",
    "\n",
    "        if show_labels:\n",
    "            for i in highlight_idx:\n",
    "                ax.annotate(str(sample_ids[i]),\n",
    "                            (x[i], y[i]),\n",
    "                            xytext=(3, 3),\n",
    "                            textcoords=\"offset points\",\n",
    "                            fontsize=8, alpha=0.9)\n",
    "\n",
    "    # Identity line\n",
    "    ax.plot([lo, hi], [lo, hi], linestyle=\"--\", linewidth=1.0, color='gray', alpha=0.9, label='Identity')\n",
    "    ax.set_xlim(lo, hi); ax.set_ylim(lo, hi); ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "    ax.set_xlabel(f\"Annealed (continued) ({METRIC_LABEL})\")\n",
    "    ax.set_ylabel(f\"Dice-only ({METRIC_LABEL})\")\n",
    "    ax.set_title(title)  # keep blank or minimal\n",
    "\n",
    "    # Text box with summary (top-left)\n",
    "    mean_delta, (ci_lo, ci_hi) = _bootstrap_mean_ci(deltas)\n",
    "    _legend_and_textbox(ax, [\n",
    "        \"Δ summary:\",\n",
    "        f\"Mean Δ: {mean_delta:.3f}\",\n",
    "        f\"95% CI: [{ci_lo:.3f}, {ci_hi:.3f}]\",\n",
    "        f\"|Δ| ≥ {thr:.3f}: {len(thr_idx)}\",\n",
    "        f\"Top-|Δ| shown: {min(top_k, deltas.size)}\",\n",
    "    ], kind=\"scatter\")\n",
    "\n",
    "    ax.grid(True, alpha=0.25)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, dpi=200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved:\", out_path)\n",
    "\n",
    "    # Return highlight dataframe for saving\n",
    "    if highlight_idx.size > 0:\n",
    "        abs_rank = pd.Series(np.abs(deltas)).rank(ascending=False, method=\"first\").astype(int)\n",
    "        return pd.DataFrame({\n",
    "            \"sample_idx\": sample_ids[highlight_idx],\n",
    "            \"annealed_continued\": x[highlight_idx],\n",
    "            \"dice_only\": y[highlight_idx],\n",
    "            \"delta_dice_minus_annealed\": deltas[highlight_idx],\n",
    "            \"abs_delta_rank\": abs_rank.iloc[highlight_idx].values\n",
    "        }).sort_values([\"abs_delta_rank\", \"delta_dice_minus_annealed\"], ascending=[True, False])\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"sample_idx\",\"annealed_continued\",\"dice_only\",\"delta_dice_minus_annealed\",\"abs_delta_rank\"\n",
    "        ])\n",
    "\n",
    "# ---------- run ----------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "for ctx, labels in CONTEXTS.items():\n",
    "    annealed_label = labels[\"annealed\"]\n",
    "    dice_label     = labels[\"dice\"]\n",
    "\n",
    "    # Pair per-sample metrics\n",
    "    x, y, sids = _pair_xy(df, annealed_label, dice_label, METRIC)\n",
    "    deltas = y - x\n",
    "\n",
    "    # Delta histogram (minimal title)\n",
    "    plot_delta_hist(\n",
    "        deltas,\n",
    "        out_dir / f\"delta_hist_{ctx}_{METRIC}.pdf\",\n",
    "        title=\"\"  # keep empty to avoid clutter; LaTeX caption will explain\n",
    "    )\n",
    "\n",
    "    # Scatter with highlights (minimal title)\n",
    "    hi_df = plot_scatter_zoom(\n",
    "        x, y, sids,\n",
    "        out_dir / f\"scatter_{ctx}_{METRIC}_zoom.pdf\",\n",
    "        title=\"\"  # keep empty\n",
    "    )\n",
    "\n",
    "    # Save highlighted table\n",
    "    hi_csv = out_dir / f\"highlighted_{ctx}_{METRIC}.csv\"\n",
    "    hi_df.to_csv(hi_csv, index=False)\n",
    "    print(\"Saved:\", hi_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a0e32c",
   "metadata": {},
   "source": [
    "## Transfer vs no transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17a4a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Pretrained vs Scratch: delta histogram + zoomed scatter with highlights ===\n",
    "# Uses existing helpers: _bootstrap_mean_ci, _legend_and_textbox, and global configs (CSV_PATH, METRIC, METRIC_LABEL,\n",
    "# TOP_K_ABS_DELTA, DELTA_THRESHOLD, SHOW_LABELS). Titles kept minimal; legend left for scatter (per previous change).\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- generic pair + plotters for pretrained vs scratch ----------\n",
    "def _pair_xy_generic(df, x_label, y_label, metric):\n",
    "    g = df.groupby([\"sample_idx\", \"model_label\"], as_index=False).agg({metric: \"mean\"})\n",
    "    wide = g[g[\"model_label\"].isin([x_label, y_label])].pivot(\n",
    "        index=\"sample_idx\", columns=\"model_label\", values=metric\n",
    "    ).dropna(subset=[x_label, y_label])\n",
    "    x = wide[x_label].to_numpy()   # Scratch\n",
    "    y = wide[y_label].to_numpy()   # Pretrained\n",
    "    return x, y, wide.index.values\n",
    "\n",
    "def plot_delta_hist_pretrained(diffs, out_path, title=\"\"):\n",
    "    mean_delta, (lo, hi) = _bootstrap_mean_ci(diffs)\n",
    "    fig, ax = plt.subplots(figsize=(6.2, 4.2), dpi=130)\n",
    "    ax.hist(diffs, bins=15, alpha=0.8, edgecolor='black')\n",
    "    ax.axvline(0, linestyle=\"--\", linewidth=1.0, color='red', alpha=0.9, label='Δ = 0')\n",
    "    ax.axvline(mean_delta, linewidth=1.2, color='blue', label=f'Mean Δ = {mean_delta:.3f}')\n",
    "    ax.set_xlabel(f\"Δ {METRIC_LABEL} (Pretrained − Scratch)\")\n",
    "    ax.set_ylabel(\"Number of Slides\")\n",
    "    ax.set_title(title)\n",
    "    _legend_and_textbox(ax, [\n",
    "        \"Summary:\",\n",
    "        f\"Mean Δ: {mean_delta:.3f}\",\n",
    "        f\"95% CI: [{lo:.3f}, {hi:.3f}]\",\n",
    "        f\"n = {diffs.size}\",\n",
    "    ], kind=\"hist\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, dpi=200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved:\", out_path)\n",
    "\n",
    "def plot_scatter_zoom_pretrained(x, y, sample_ids, out_path, title=\"\", pad=0.02,\n",
    "                                 top_k=TOP_K_ABS_DELTA, thr=DELTA_THRESHOLD, show_labels=SHOW_LABELS):\n",
    "    xmin, xmax = x.min(), x.max()\n",
    "    ymin, ymax = y.min(), y.max()\n",
    "    lo = min(xmin, ymin); hi = max(xmax, ymax)\n",
    "    span = max(hi - lo, 1e-6)\n",
    "    lo -= pad*span; hi += pad*span\n",
    "\n",
    "    deltas = y - x  # Pretrained − Scratch\n",
    "    idx_sorted = np.argsort(-np.abs(deltas))\n",
    "    top_idx = set(idx_sorted[:top_k])\n",
    "    thr_idx = set(np.where(np.abs(deltas) >= thr)[0])\n",
    "    highlight_idx = np.array(sorted(top_idx.union(thr_idx)), dtype=int)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6.2, 6.2), dpi=130)\n",
    "    ax.scatter(x, y, alpha=0.25, edgecolor=\"none\", label=\"All slides\")\n",
    "\n",
    "    if highlight_idx.size > 0:\n",
    "        imp = highlight_idx[deltas[highlight_idx] > 0]\n",
    "        deg = highlight_idx[deltas[highlight_idx] < 0]\n",
    "        if imp.size:\n",
    "            ax.scatter(x[imp], y[imp], alpha=0.9, edgecolor=\"black\", linewidths=0.6,\n",
    "                       label=f\"Highlighted (Δ>0, n={imp.size})\")\n",
    "        if deg.size:\n",
    "            ax.scatter(x[deg], y[deg], alpha=0.9, edgecolor=\"black\", linewidths=0.6,\n",
    "                       label=f\"Highlighted (Δ<0, n={deg.size})\", marker=\"s\")\n",
    "        if show_labels:\n",
    "            for i in highlight_idx:\n",
    "                ax.annotate(str(sample_ids[i]), (x[i], y[i]),\n",
    "                            xytext=(3, 3), textcoords=\"offset points\",\n",
    "                            fontsize=8, alpha=0.9)\n",
    "\n",
    "    ax.plot([lo, hi], [lo, hi], linestyle=\"--\", linewidth=1.0, color='gray', alpha=0.9, label='Identity')\n",
    "    ax.set_xlim(lo, hi); ax.set_ylim(lo, hi); ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "    ax.set_xlabel(f\"Scratch ({METRIC_LABEL})\")\n",
    "    ax.set_ylabel(f\"Pretrained ({METRIC_LABEL})\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "    mean_delta, (ci_lo, ci_hi) = _bootstrap_mean_ci(deltas)\n",
    "    _legend_and_textbox(ax, [\n",
    "        \"Δ summary:\",\n",
    "        f\"Mean Δ: {mean_delta:.3f}\",\n",
    "        f\"95% CI: [{ci_lo:.3f}, {ci_hi:.3f}]\",\n",
    "        f\"|Δ| ≥ {thr:.3f}: {len(thr_idx)}\",\n",
    "        f\"Top-|Δ| shown: {min(top_k, deltas.size)}\",\n",
    "    ], kind=\"scatter\")\n",
    "\n",
    "    ax.grid(True, alpha=0.25)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, dpi=200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved:\", out_path)\n",
    "\n",
    "    if highlight_idx.size > 0:\n",
    "        abs_rank = pd.Series(np.abs(deltas)).rank(ascending=False, method=\"first\").astype(int)\n",
    "        return pd.DataFrame({\n",
    "            \"sample_idx\": sample_ids[highlight_idx],\n",
    "            \"scratch\": x[highlight_idx],\n",
    "            \"pretrained\": y[highlight_idx],\n",
    "            \"delta_pretrained_minus_scratch\": deltas[highlight_idx],\n",
    "            \"abs_delta_rank\": abs_rank.iloc[highlight_idx].values\n",
    "        }).sort_values([\"abs_delta_rank\", \"delta_pretrained_minus_scratch\"], ascending=[True, False])\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"sample_idx\",\"scratch\",\"pretrained\",\"delta_pretrained_minus_scratch\",\"abs_delta_rank\"\n",
    "        ])\n",
    "\n",
    "# ---------- run comparisons ----------\n",
    "out_dir = Path(\"figs/pretrained_vs_scratch\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "COMPARISONS = [\n",
    "    # continued (annealed) variants\n",
    "    {\"ctx\": \"nf\", \"variant\": \"continued\", \"scratch\": \"scratch_continued\", \"pretrained\": \"nf_continued\"},\n",
    "    {\"ctx\": \"th\", \"variant\": \"continued\", \"scratch\": \"scratch_continued\", \"pretrained\": \"th_continued\"},\n",
    "    # dice-only variants\n",
    "    {\"ctx\": \"nf\", \"variant\": \"dice\", \"scratch\": \"scratch_dice_only\", \"pretrained\": \"nf_dice_only\"},\n",
    "    {\"ctx\": \"th\", \"variant\": \"dice\", \"scratch\": \"scratch_dice_only\", \"pretrained\": \"th_dice_only\"},\n",
    "]\n",
    "\n",
    "for c in COMPARISONS:\n",
    "    x, y, sids = _pair_xy_generic(df, c[\"scratch\"], c[\"pretrained\"], METRIC)\n",
    "    if x.size == 0:\n",
    "        print(f\"Skipping {c['ctx']} ({c['variant']}): no overlapping samples.\")\n",
    "        continue\n",
    "\n",
    "    deltas = y - x\n",
    "\n",
    "    # Δ histogram (minimal title)\n",
    "    plot_delta_hist_pretrained(\n",
    "        deltas,\n",
    "        out_dir / f\"delta_hist_{c['ctx']}_{c['variant']}_{METRIC}.pdf\",\n",
    "        title=\"\"\n",
    "    )\n",
    "\n",
    "    # Scatter with highlights (minimal title)\n",
    "    hi_df = plot_scatter_zoom_pretrained(\n",
    "        x, y, sids,\n",
    "        out_dir / f\"scatter_{c['ctx']}_{c['variant']}_{METRIC}_zoom.pdf\",\n",
    "        title=\"\"\n",
    "    )\n",
    "\n",
    "    # Save highlighted table\n",
    "    hi_csv = out_dir / f\"highlighted_{c['ctx']}_{c['variant']}_{METRIC}.csv\"\n",
    "    hi_df.to_csv(hi_csv, index=False)\n",
    "    print(\"Saved:\", hi_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cc1461",
   "metadata": {},
   "source": [
    "## Overview table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d0c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Overview table of top models, including both *_continued and *_annealed (no files saved)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Config ---\n",
    "CSV_PATH = \"per_sample_metrics.csv\"   # adjust if needed\n",
    "RNG_SEED = 42\n",
    "N_BOOT   = 10_000\n",
    "\n",
    "# --- Load ---\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Metrics available\n",
    "METRICS = [m for m in [\"soft_dice\", \"cldice\"] if m in df.columns]\n",
    "if not METRICS:\n",
    "    raise ValueError(\"No supported metrics found. Expected 'soft_dice' and/or 'cldice' columns in CSV.\")\n",
    "\n",
    "# Include both *_continued and *_annealed, plus dice-only variants\n",
    "preferred_models = [\n",
    "    \"scratch_continued\", \"scratch_annealed\",\n",
    "    \"nf_continued\",      \"nf_annealed\",\n",
    "    \"th_continued\",      \"th_annealed\",\n",
    "    \"scratch_dice_only\", \"nf_dice_only\", \"th_dice_only\",\n",
    "]\n",
    "\n",
    "present = set(df[\"model_label\"].unique())\n",
    "\n",
    "# Keep only those that exist, in the preferred order; optionally include baseline at the end if present\n",
    "models = [m for m in preferred_models if m in present]\n",
    "if \"baseline\" in present and \"baseline\" not in models:\n",
    "    models.append(\"baseline\")\n",
    "\n",
    "# If none matched, use all labels\n",
    "if not models:\n",
    "    models = sorted(present)\n",
    "\n",
    "# --- Helpers ---\n",
    "def summarize_metric(df_in: pd.DataFrame, metric: str, model_label: str, n_boot: int = N_BOOT, seed: int = RNG_SEED):\n",
    "    # Aggregate per-sample first (avoid duplicates)\n",
    "    g = (df_in[df_in[\"model_label\"] == model_label]\n",
    "         .groupby(\"sample_idx\", as_index=False)[metric].mean())\n",
    "    x = g[metric].to_numpy()\n",
    "    n = x.size\n",
    "    if n == 0:\n",
    "        return {\n",
    "            \"Metric\": metric, \"Model\": model_label,\n",
    "            \"Mean [95% CI]\": \"—\", \"Median [IQR]\": \"—\", \"n\": 0\n",
    "        }\n",
    "    # Bootstrap CI for the mean\n",
    "    rng = np.random.default_rng(seed)\n",
    "    boots = rng.choice(x, size=(n_boot, n), replace=True).mean(axis=1)\n",
    "    ci_low = np.percentile(boots, 2.5)\n",
    "    ci_high = np.percentile(boots, 97.5)\n",
    "\n",
    "    mean = x.mean()\n",
    "    median = np.median(x)\n",
    "    q1 = np.percentile(x, 25)\n",
    "    q3 = np.percentile(x, 75)\n",
    "\n",
    "    return {\n",
    "        \"Metric\": metric,\n",
    "        \"Model\": model_label,\n",
    "        \"Mean [95% CI]\": f\"{mean:.3f} [{ci_low:.3f}, {ci_high:.3f}]\",\n",
    "        \"Median [IQR]\": f\"{median:.3f} [{q1:.3f}, {q3:.3f}]\",\n",
    "        \"n\": int(n)\n",
    "    }\n",
    "\n",
    "# --- Build + show table ---\n",
    "rows = []\n",
    "for metric in METRICS:\n",
    "    for m in models:\n",
    "        rows.append(summarize_metric(df, metric, m))\n",
    "\n",
    "summary_df = pd.DataFrame(rows)\n",
    "\n",
    "# Order rows: by metric then preferred model order\n",
    "summary_df[\"Model\"] = pd.Categorical(summary_df[\"Model\"], categories=models, ordered=True)\n",
    "summary_df = summary_df.sort_values([\"Metric\", \"Model\"]).reset_index(drop=True)\n",
    "\n",
    "# Display in Jupyter\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b0ce5b",
   "metadata": {},
   "source": [
    "## Stringyness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036cf8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Stringyness perspective: Paired Δ-clDice at matched Soft-Dice ===\n",
    "# For each context, compare clDice(annealed) − clDice(dice-only)\n",
    "# Plots:\n",
    "#   1) Δ-clDice histogram with bootstrap 95% CI (legend top-right, textbox below legend)\n",
    "#   2) Soft-Dice scatter (x = dice-only, y = annealed) colored by Δ-clDice (legend upper-left, textbox below legend)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "CSV_PATH = \"per_sample_metrics.csv\"\n",
    "# Use \"*_continued\" as your annealed variant:\n",
    "CONTEXTS = {\n",
    "    \"scratch\": {\"annealed\": \"scratch_continued\", \"dice\": \"scratch_dice_only\"},\n",
    "    \"nf\":      {\"annealed\": \"nf_continued\",      \"dice\": \"nf_dice_only\"},\n",
    "    \"th\":      {\"annealed\": \"th_continued\",      \"dice\": \"th_dice_only\"},\n",
    "}\n",
    "N_BOOT = 10_000\n",
    "SEED   = 42\n",
    "SAVE   = True\n",
    "OUTDIR = Path(\"figs/stringyness\"); OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "# ------------------------------------------------\n",
    "\n",
    "# ---------- helpers (re-use if already defined) ----------\n",
    "def _bootstrap_mean_ci(x, n_boot=N_BOOT, ci=95, seed=SEED):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    boots = rng.choice(x, size=(n_boot, x.size), replace=True).mean(axis=1)\n",
    "    lo = np.percentile(boots, (100-ci)/2)\n",
    "    hi = np.percentile(boots, 100-(100-ci)/2)\n",
    "    return float(x.mean()), (float(lo), float(hi))\n",
    "\n",
    "def _pair_metrics(df, annealed_label, dice_label):\n",
    "    \"\"\"Return per-sample paired soft_dice (annealed/dice) and cldice (annealed/dice).\"\"\"\n",
    "    need_cols = {\"sample_idx\", \"model_label\", \"soft_dice\", \"cldice\"}\n",
    "    missing = need_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"CSV missing columns: {missing}\")\n",
    "\n",
    "    sub = df[df[\"model_label\"].isin([annealed_label, dice_label])]\n",
    "    g = sub.groupby([\"sample_idx\", \"model_label\"], as_index=False)[[\"soft_dice\",\"cldice\"]].mean()\n",
    "\n",
    "    # Pivot to MultiIndex columns: (metric, model_label)\n",
    "    wide = g.pivot(index=\"sample_idx\", columns=\"model_label\", values=[\"soft_dice\",\"cldice\"])\n",
    "\n",
    "    # ✅ Correctly flatten: \"<metric>_<model_label>\"\n",
    "    wide.columns = [f\"{metric}_{label}\" for metric, label in wide.columns.to_flat_index()]\n",
    "\n",
    "    # Keep only samples present for both models\n",
    "    cols = [\n",
    "        f\"soft_dice_{annealed_label}\", f\"soft_dice_{dice_label}\",\n",
    "        f\"cldice_{annealed_label}\",    f\"cldice_{dice_label}\",\n",
    "    ]\n",
    "    missing_cols = [c for c in cols if c not in wide.columns]\n",
    "    if missing_cols:\n",
    "        raise KeyError(f\"Expected after pivot, not found: {missing_cols}. Got: {list(wide.columns)}\")\n",
    "\n",
    "    wide = wide.dropna(subset=cols).reset_index()\n",
    "    return wide\n",
    "\n",
    "\n",
    "def _textbox(ax, lines, x, y, align=(\"left\",\"top\")):\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.85)\n",
    "    ha = \"left\" if align[0]==\"left\" else \"right\"\n",
    "    va = \"top\"  if align[1]==\"top\"  else \"bottom\"\n",
    "    ax.text(x, y, \"\\n\".join(lines), transform=ax.transAxes, fontsize=9, ha=ha, va=va, bbox=props)\n",
    "\n",
    "# ---------- plotting ----------\n",
    "def plot_delta_hist(deltas, ctx, save=SAVE):\n",
    "    mean_delta, (lo, hi) = _bootstrap_mean_ci(deltas)\n",
    "    fig, ax = plt.subplots(figsize=(6.2, 4.2), dpi=130)\n",
    "    ax.hist(deltas, bins=15, alpha=0.85, edgecolor=\"black\")\n",
    "    ax.axvline(0, linestyle=\"--\", linewidth=1.0, color=\"red\", alpha=0.9, label=\"Δ = 0\")\n",
    "    ax.axvline(mean_delta, linewidth=1.2, color=\"blue\", label=f\"Mean Δ = {mean_delta:.3f}\")\n",
    "    ax.set_xlabel(\"Δ clDice (annealed − dice-only)\")\n",
    "    ax.set_ylabel(\"Number of slides\")\n",
    "    ax.set_title(\"\")  # minimal title (caption will explain)\n",
    "    ax.legend(loc=\"upper right\", frameon=True)\n",
    "\n",
    "    # Textbox below the legend (top-right area)\n",
    "    _textbox(ax, [\n",
    "        f\"Mean Δ: {mean_delta:.3f}\",\n",
    "        f\"95% CI: [{lo:.3f}, {hi:.3f}]\",\n",
    "        f\"n = {deltas.size}\",\n",
    "    ], x=0.98, y=0.70, align=(\"right\",\"top\"))\n",
    "\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    fig.tight_layout()\n",
    "    if save:\n",
    "        fp = OUTDIR / f\"delta_cldice_hist_{ctx}.pdf\"\n",
    "        fig.savefig(fp, dpi=200, bbox_inches=\"tight\")\n",
    "        print(\"Saved:\", fp)\n",
    "    plt.show()\n",
    "\n",
    "def plot_softdice_scatter_colored(pair_df, annealed_label, dice_label, ctx, save=SAVE):\n",
    "    x = pair_df[f\"soft_dice_{dice_label}\"].to_numpy()\n",
    "    y = pair_df[f\"soft_dice_{annealed_label}\"].to_numpy()\n",
    "    deltas = pair_df[f\"cldice_{annealed_label}\"].to_numpy() - pair_df[f\"cldice_{dice_label}\"].to_numpy()\n",
    "\n",
    "    # Range padding and identity\n",
    "    pad = 0.02\n",
    "    xmin, xmax = x.min(), x.max()\n",
    "    ymin, ymax = y.min(), y.max()\n",
    "    lo = min(xmin, ymin); hi = max(xmax, ymax)\n",
    "    span = max(hi - lo, 1e-6)\n",
    "    lo -= pad*span; hi += pad*span\n",
    "\n",
    "    # Diverging color centered at 0 for ΔclDice\n",
    "    norm = mcolors.TwoSlopeNorm(vmin=np.min(deltas), vcenter=0.0, vmax=np.max(deltas))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6.2, 6.2), dpi=130)\n",
    "    sc = ax.scatter(x, y, c=deltas, cmap=\"coolwarm\", norm=norm, alpha=0.9, edgecolor=\"none\")\n",
    "    ax.plot([lo, hi], [lo, hi], linestyle=\"--\", linewidth=1.0, color=\"gray\", alpha=0.9, label=\"Identity\")\n",
    "\n",
    "    ax.set_xlim(lo, hi); ax.set_ylim(lo, hi); ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "    ax.set_xlabel(\"Soft-Dice (dice-only)\")\n",
    "    ax.set_ylabel(\"Soft-Dice (annealed)\")\n",
    "    ax.set_title(\"\")\n",
    "\n",
    "    # Legend upper-left, textbox below legend (top-left)\n",
    "    ax.legend(loc=\"upper left\", frameon=True)\n",
    "    mean_delta, (ci_lo, ci_hi) = _bootstrap_mean_ci(deltas)\n",
    "    _textbox(ax, [\n",
    "        f\"ΔclDice mean: {mean_delta:.3f}\",\n",
    "        f\"95% CI: [{ci_lo:.3f}, {ci_hi:.3f}]\",\n",
    "        f\"n = {deltas.size}\",\n",
    "    ], x=0.02, y=0.70, align=(\"left\",\"top\"))\n",
    "\n",
    "    # Colorbar for ΔclDice\n",
    "    cbar = plt.colorbar(sc, ax=ax, shrink=0.9, pad=0.02)\n",
    "    cbar.set_label(\"Δ clDice (annealed − dice-only)\")\n",
    "\n",
    "    ax.grid(True, alpha=0.25)\n",
    "    fig.tight_layout()\n",
    "    if save:\n",
    "        fp = OUTDIR / f\"softdice_scatter_colored_{ctx}.pdf\"\n",
    "        fig.savefig(fp, dpi=200, bbox_inches=\"tight\")\n",
    "        print(\"Saved:\", fp)\n",
    "    plt.show()\n",
    "\n",
    "# ---------- run ----------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "for ctx, lbls in CONTEXTS.items():\n",
    "    annealed_label = lbls[\"annealed\"]\n",
    "    dice_label     = lbls[\"dice\"]\n",
    "\n",
    "    paired = _pair_metrics(df, annealed_label, dice_label)\n",
    "    if paired.empty:\n",
    "        print(f\"[{ctx}] No overlapping samples for {annealed_label} vs {dice_label}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 1) Δ-clDice histogram\n",
    "    deltas = paired[f\"cldice_{annealed_label}\"].to_numpy() - paired[f\"cldice_{dice_label}\"].to_numpy()\n",
    "    plot_delta_hist(deltas, ctx)\n",
    "\n",
    "    # 2) Soft-Dice scatter colored by Δ-clDice\n",
    "    plot_softdice_scatter_colored(paired, annealed_label, dice_label, ctx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed3e64d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
